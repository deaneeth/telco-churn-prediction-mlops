{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324ff9a5",
   "metadata": {},
   "source": [
    "## Telco Customer Churn - Part 4 :  Production-Ready Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58a950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11d6d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblearn version: 0.14.0\n",
      "scikit-learn version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "import sklearn\n",
    "\n",
    "print(f\"imblearn version: {imblearn.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850f365",
   "metadata": {},
   "source": [
    "#### 0. Setup: imports, config, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8407cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from src.utils import load_config, get_logger, ensure_dir\n",
    "from src.data_ingestion import DataIngestion\n",
    "from src.handle_missing_values import MissingValueHandler\n",
    "from src.outlier_detection import iqr_outliers, zscore_outliers, remove_outliers\n",
    "from src.data_splitter import stratified_split\n",
    "from src.data_pipeline import build_preprocessing_pipeline\n",
    "from src.data_splitter import stratified_split\n",
    "from src.model_building import build_model\n",
    "from src.model_training import Trainer\n",
    "from src.model_evaluation import evaluate, threshold_search, compute_business_cost\n",
    "from src.model_inference import InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a44ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project root\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'config.yaml'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define trained models directory\n",
    "TRAINED_MODELS_DIR = PROJECT_ROOT / 'models' / 'trained_models'\n",
    "TRAINED_MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f865126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported src modules successfully\n"
     ]
    }
   ],
   "source": [
    "# import modules from src\n",
    "logger = get_logger('Part4_Maximum')\n",
    "print('Imported src modules successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fd3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback config\n",
      "{'business': {'cost_fp': 10.0, 'months_lost': 6},\n",
      " 'data': {'output_dir': 'artifacts',\n",
      "          'raw_path': 'src/data/processed/telco-customer-churn_cleaned.csv'},\n",
      " 'model': {'catboost': {'iterations': 300},\n",
      "           'rf': {'max_depth': 12, 'min_samples_split': 5, 'n_estimators': 300},\n",
      "           'xgb': {'learning_rate': 0.05, 'n_estimators': 300}},\n",
      " 'processing': {'categorical_impute': 'most_frequent',\n",
      "                'encoder': 'onehot',\n",
      "                'numeric_impute': 'median',\n",
      "                'scaler': 'standard'},\n",
      " 'seed': 42,\n",
      " 'training': {'cv_folds': 5, 'scoring': 'roc_auc', 'test_size': 0.2}}\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "if CONFIG_PATH.exists():\n",
    "    config = load_config(str(CONFIG_PATH))\n",
    "else:\n",
    "    # fallback\n",
    "    config = {\n",
    "        'seed': 42,\n",
    "        'data': {'raw_path': '/mnt/data/telco-customer-churn_cleaned.csv', 'output_dir': str(ARTIFACTS_DIR)},\n",
    "        'processing': {'numeric_impute': 'median', 'categorical_impute': 'most_frequent', 'scaler': 'standard', 'encoder': 'onehot'},\n",
    "        'model': {'rf': {'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 5}, 'xgb': {'n_estimators': 300, 'learning_rate': 0.05}, 'catboost': {'iterations': 300}},\n",
    "        'training': {'cv_folds': 5, 'test_size': 0.2, 'scoring': 'roc_auc'},\n",
    "        'business': {'cost_fp': 10.0, 'months_lost': 6}\n",
    "    }\n",
    "print('Using fallback config')\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a10457",
   "metadata": {},
   "source": [
    "### 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "474c9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 15:16:19,026 - data_ingestion - INFO - Loading data from src/data/processed/telco-customer-churn_cleaned.csv\n",
      "2025-08-23 15:16:19,049 - data_ingestion - INFO - Loaded dataframe shape: (7043, 24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from src/data/processed/telco-customer-churn_cleaned.csv\n",
      "\n",
      "Loaded DataFrame shape: (7043, 24)\n",
      "\n",
      "Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn', 'tenure_group', 'services_count', 'avg_charge_per_month']\n"
     ]
    }
   ],
   "source": [
    "data_path = config['data']['raw_path']\n",
    "print('Loading data from', data_path)\n",
    "\n",
    "ingestor = DataIngestion(data_path)\n",
    "df = ingestor.load_csv()\n",
    "print('\\nLoaded DataFrame shape:', df.shape)\n",
    "print('\\nColumns:', df.columns.tolist()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755af1a8",
   "metadata": {},
   "source": [
    "### 2. Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d360c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before:\n",
      "TotalCharges            11\n",
      "avg_charge_per_month    11\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMissing values before:')\n",
    "print(df.isnull().sum()[lambda s: s>0])\n",
    "\n",
    "mv_handler = MissingValueHandler(numeric_strategy=config['processing']['numeric_impute'], categorical_strategy=config['processing']['categorical_impute'])\n",
    "# fit-transform on entire dataframe\n",
    "mv_handler.fit(df)\n",
    "df_imputed = mv_handler.transform(df)\n",
    "print('\\nMissing values after imputation:')\n",
    "print(df_imputed.isnull().sum()[lambda s: s>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee11da",
   "metadata": {},
   "source": [
    "### 3. Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c411004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'services_count', 'avg_charge_per_month']\n",
      "\n",
      "IQR outlier counts (example):\n",
      "{'tenure': 0, 'MonthlyCharges': 0, 'TotalCharges': 0}\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
    "print('Numeric cols:', numeric_cols)\n",
    "\n",
    "outlier_counts = {c: iqr_outliers(df_imputed[c]).sum() for c in ['tenure','MonthlyCharges','TotalCharges'] if c in df_imputed.columns}\n",
    "print('\\nIQR outlier counts (example):')\n",
    "print(outlier_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366481d6",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60436f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "df_fe = df_imputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c39b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenure_category\n",
    "def tenure_category(t):\n",
    "    if t <= 12:\n",
    "        return 'New'\n",
    "    elif t <= 48:\n",
    "        return 'Established'\n",
    "    else:\n",
    "        return 'Loyal'\n",
    "\n",
    "\n",
    "if 'tenure' in df_fe.columns:\n",
    "    df_fe['tenure_category'] = df_fe['tenure'].apply(tenure_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f135350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# services_count\n",
    "SERVICES = ['PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n",
    "service_map = {'Yes':1,'No':0,'No phone service':0,'No internet service':0}\n",
    "for s in SERVICES:\n",
    "    if s not in df_fe.columns:\n",
    "        # if some services missing, skip\n",
    "        pass\n",
    "    \n",
    "service_flags = df_fe[SERVICES].map(lambda x: service_map.get(x,0))\n",
    "df_fe['services_count'] = service_flags.sum(axis=1)\n",
    "df_fe['service_adoption_score'] = df_fe['services_count'] / len(SERVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38c7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg charge per active service\n",
    "if 'MonthlyCharges' in df_fe.columns:\n",
    "    df_fe['avg_charge_per_service'] = df_fe['MonthlyCharges'] / df_fe['services_count'].replace(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8fd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payment proxies\n",
    "if 'PaymentMethod' in df_fe.columns:\n",
    "    df_fe['is_electronic_check'] = df_fe['PaymentMethod'].str.contains('Electronic check', na=False).astype(int)\n",
    "if 'PaperlessBilling' in df_fe.columns:\n",
    "    df_fe['paperless_flag'] = df_fe['PaperlessBilling'].map({'Yes':1,'No':0}).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d46a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature engineering completed. Sample new columns:\n",
      "  tenure_category  services_count  service_adoption_score  \\\n",
      "0             New               1                0.111111   \n",
      "1     Established               3                0.333333   \n",
      "2             New               3                0.333333   \n",
      "3     Established               3                0.333333   \n",
      "4             New               1                0.111111   \n",
      "\n",
      "   avg_charge_per_service  is_electronic_check  paperless_flag  \n",
      "0               29.850000                    1               1  \n",
      "1               18.983333                    0               0  \n",
      "2               17.950000                    0               1  \n",
      "3               14.100000                    0               0  \n",
      "4               70.700000                    1               1  \n"
     ]
    }
   ],
   "source": [
    "# contract-tenure interaction\n",
    "if 'Contract' in df_fe.columns and 'tenure_category' in df_fe.columns:\n",
    "    df_fe['contract_tenure_interaction'] = df_fe['Contract'].astype(str) + '_' + df_fe['tenure_category'].astype(str)\n",
    "    \n",
    "print('\\nFeature engineering completed. Sample new columns:')\n",
    "print(df_fe[['tenure_category','services_count','service_adoption_score','avg_charge_per_service','is_electronic_check','paperless_flag']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c4cf3",
   "metadata": {},
   "source": [
    "### 5. Train/Validation/Test Split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f429b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes -> X_train: (4225, 29) X_val: (1409, 29) X_test: (1409, 29)\n"
     ]
    }
   ],
   "source": [
    "# prepare X/y\n",
    "if 'Churn' in df_fe.columns:\n",
    "    df_fe['Churn_flag'] = df_fe['Churn'].map({'No':0,'Yes':1}).astype(int)\n",
    "    y = df_fe['Churn_flag']\n",
    "    X = df_fe.drop(columns=['Churn','Churn_flag'] if 'Churn' in df_fe.columns else ['Churn_flag'])\n",
    "else:\n",
    "    raise ValueError('Churn column missing — ensure cleaned dataset includes Churn')\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(X, y, test_size=config['training'].get('test_size',0.2), val_size=0.2, seed=config['seed'])\n",
    "print('\\nShapes -> X_train:', X_train.shape, 'X_val:', X_val.shape, 'X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8db738",
   "metadata": {},
   "source": [
    "### 6. Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44942324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'services_count', 'avg_charge_per_month', 'service_adoption_score', 'avg_charge_per_service', 'is_electronic_check', 'paperless_flag']\n",
      "Categorical features (sample): ['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'tenure_group', 'tenure_category', 'contract_tenure_interaction']\n",
      "\n",
      "Preprocessor created:\n",
      "ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['SeniorCitizen', 'tenure', 'MonthlyCharges',\n",
      "                                  'TotalCharges', 'services_count',\n",
      "                                  'avg_charge_per_month',\n",
      "                                  'service_adoption_score',\n",
      "                                  'avg_charge_per_service',\n",
      "                                  'is_electronic_check', 'paperless_flag']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('impute...\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False))]),\n",
      "                                 ['customerID', 'gender', 'Partner',\n",
      "                                  'Dependents', 'PhoneService', 'MultipleLines',\n",
      "                                  'InternetService', 'OnlineSecurity',\n",
      "                                  'OnlineBackup', 'DeviceProtection',\n",
      "                                  'TechSupport', 'StreamingTV',\n",
      "                                  'StreamingMovies', 'Contract',\n",
      "                                  'PaperlessBilling', 'PaymentMethod',\n",
      "                                  'tenure_group', 'tenure_category',\n",
      "                                  'contract_tenure_interaction'])])\n",
      "\n",
      "Transformed sample shape: (5, 60)\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric & categorical features from X_train\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print('Numeric features:', numeric_features[:20])\n",
    "print('Categorical features (sample):', categorical_features[:20])\n",
    "\n",
    "# Create numeric transformer based on config\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=config['processing']['numeric_impute'])),\n",
    "    ('scaler', StandardScaler() if config['processing']['scaler'] == 'standard' else \n",
    "             (MinMaxScaler() if config['processing']['scaler'] == 'minmax' else RobustScaler()))\n",
    "])\n",
    "\n",
    "# Create categorical transformer with direct OneHotEncoder initialization\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=config['processing']['categorical_impute'], fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print('\\nPreprocessor created:')\n",
    "print(preprocessor)\n",
    "\n",
    "# Demonstrate transform on a small sample\n",
    "sample_transformed = preprocessor.fit_transform(X_train.iloc[:5])\n",
    "print('\\nTransformed sample shape:', getattr(sample_transformed, 'shape', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ae32",
   "metadata": {},
   "source": [
    "### 7. Build & Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf23365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model: RandomForestClassifier(class_weight='balanced', max_depth=12,\n",
      "                       min_samples_split=5, n_estimators=300, n_jobs=-1,\n",
      "                       random_state=42)\n",
      "XGB model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, feature_weights=None, gamma=None,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=300, n_jobs=-1,\n",
      "              num_parallel_tree=None, ...)\n",
      "CatBoost model: <catboost.core.CatBoostClassifier object at 0x000001F1CC5B38D0>\n"
     ]
    }
   ],
   "source": [
    "# Build models\n",
    "rf = build_model('random_forest', config)\n",
    "\n",
    "print('RandomForest model:', rf)\n",
    "try:\n",
    "    xgb = build_model('xgboost', config)\n",
    "    print('XGB model:', xgb)\n",
    "except Exception as e:\n",
    "    print('XGBoost not available:', e)\n",
    "try:\n",
    "    cat = build_model('catboost', config)\n",
    "    print('CatBoost model:', cat)\n",
    "except Exception as e:\n",
    "    print('CatBoost not available:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bde7c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained and saved RandomForest pipeline to: E:\\Projects\\mini-project\\models\\trained_models\n"
     ]
    }
   ],
   "source": [
    "# Train RandomForest with Trainer (no hyperparam search for speed in demo)\n",
    "artifacts_dir = str(TRAINED_MODELS_DIR)  # Use trained_models directory instead of artifacts\n",
    "trainer = Trainer(preprocessor, rf, config, artifacts_dir)\n",
    "rf_pipeline, _ = trainer.fit(X_train, y_train, param_distributions=None)\n",
    "print('\\nTrained and saved RandomForest pipeline to:', artifacts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76452f",
   "metadata": {},
   "source": [
    "### 8. Evaluate Models (imbalanced-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00e56e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load E:\\Projects\\mini-project\\models\\trained_models\\RandomForestClassifier_pipeline.joblib\n",
      "Loaded pipeline: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['SeniorCitizen', 'tenure',\n",
      "                                                   'MonthlyCharges',\n",
      "                                                   'TotalCharges',\n",
      "                                                   'services_count',\n",
      "                                                   'avg_charge_per_month',\n",
      "                                                   'service_adoption_score',\n",
      "                                                   'avg_charge_per_service',\n",
      "                                                   'is_electronic_check',\n",
      "                                                   'paperless_flag'])...\n",
      "                                                   'InternetService',\n",
      "                                                   'OnlineSecurity',\n",
      "                                                   'OnlineBackup',\n",
      "                                                   'DeviceProtection',\n",
      "                                                   'TechSupport', 'StreamingTV',\n",
      "                                                   'StreamingMovies',\n",
      "                                                   'Contract',\n",
      "                                                   'PaperlessBilling',\n",
      "                                                   'PaymentMethod',\n",
      "                                                   'tenure_group',\n",
      "                                                   'tenure_category',\n",
      "                                                   'contract_tenure_interaction'])])),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(class_weight='balanced', max_depth=12,\n",
      "                                        min_samples_split=5, n_estimators=300,\n",
      "                                        n_jobs=-1, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# load trained pipeline artifact\n",
    "rf_artifact_path = os.path.join(artifacts_dir, f\"{type(rf).__name__}_pipeline.joblib\")\n",
    "print('Attempting to load', rf_artifact_path)\n",
    "rf_loaded = load(rf_artifact_path)\n",
    "print('Loaded pipeline:', rf_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf5c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest metrics:\n",
      "{'roc_auc': 0.8389198894313983, 'pr_auc': 0.6368315308530927, 'precision': 0.5207207207207207, 'recall': 0.7727272727272727, 'f1': 0.6221743810548978, 'confusion_matrix': array([[769, 266],\n",
      "       [ 85, 289]], dtype=int64), 'best_threshold': 0.49144896231389945, 'best_f1': 0.6277836686624196}\n"
     ]
    }
   ],
   "source": [
    "# get probs on test set\n",
    "probs_rf = rf_loaded.predict_proba(X_test)[:,1]\n",
    "metrics_rf = evaluate(y_test, probs_rf, threshold=0.5)\n",
    "best_thresh, best_f1 = threshold_search(y_test, probs_rf)\n",
    "metrics_rf['best_threshold'] = best_thresh\n",
    "metrics_rf['best_f1'] = best_f1\n",
    "print('\\nRandomForest metrics:')\n",
    "print(metrics_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96bcc1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized InferenceService with model from: E:\\Projects\\mini-project\\models\\trained_models\\RandomForestClassifier_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# Initialize the InferenceService with the loaded model\n",
    "inference_service = InferenceService(rf_artifact_path)\n",
    "print(\"Initialized InferenceService with model from:\", rf_artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a246cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Business cost at best threshold: {'tn': 762, 'fp': 273, 'fn': 78, 'tp': 296, 'total_cost': 33038.47207156041}\n"
     ]
    }
   ],
   "source": [
    "# compute business cost at best threshold\n",
    "cost_fp = config['business'].get('cost_fp',10.0)\n",
    "months_lost = config['business'].get('months_lost',6)\n",
    "avg_monthly = df_fe['MonthlyCharges'].mean()\n",
    "cost_fn = avg_monthly * months_lost\n",
    "business = compute_business_cost(y_test, probs_rf, best_thresh, cost_fp, cost_fn)\n",
    "print('\\nBusiness cost at best threshold:', business)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa041a29",
   "metadata": {},
   "source": [
    "### 9. Save results & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28dc980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved performance summary to E:\\Projects\\mini-project\\src\\data\\external\\part4_performance_summary.csv\n",
      "\n",
      "Model artifact saved in: E:\\Projects\\mini-project\\models\\trained_models\n",
      "File: RandomForestClassifier_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "perf_summary = pd.DataFrame([{\n",
    "    'model':'RandomForest',\n",
    "    'roc_auc':metrics_rf['roc_auc'],\n",
    "    'pr_auc':metrics_rf['pr_auc'],\n",
    "    'precision':metrics_rf['precision'],\n",
    "    'recall':metrics_rf['recall'],\n",
    "    'f1':metrics_rf['f1'],\n",
    "    'best_threshold':metrics_rf['best_threshold'],\n",
    "    'best_f1':metrics_rf['best_f1'],\n",
    "    'artifact_path': rf_artifact_path\n",
    "}])\n",
    "\n",
    "# Create data/external directory if it doesn't exist\n",
    "data_external_dir = PROJECT_ROOT / 'src' / 'data' / 'external'\n",
    "os.makedirs(data_external_dir, exist_ok=True)\n",
    "\n",
    "# Save to data/external directory\n",
    "perf_summary.to_csv(data_external_dir / 'part4_performance_summary.csv', index=False)\n",
    "print('Saved performance summary to', data_external_dir / 'part4_performance_summary.csv')\n",
    "print('\\nModel artifact saved in:', artifacts_dir)\n",
    "print('File:', f\"{type(rf).__name__}_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f5e08",
   "metadata": {},
   "source": [
    "### 10. Inference Demo (single sample and batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7765dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single sample -> probability: 0.16105109932931244 prediction_at_best_threshold: 0\n",
      "\n",
      "Batch probs (first 10): [0.1610511  0.72711156 0.27533644 0.57941758 0.17603401 0.6758816\n",
      " 0.59797809 0.22466842 0.0588652  0.62657955]\n"
     ]
    }
   ],
   "source": [
    "# single-sample: take first row from X_test\n",
    "single_row = X_test.iloc[[0]]\n",
    "prob = inference_service.predict_proba(single_row)[0]\n",
    "pred = inference_service.predict(single_row, threshold=metrics_rf['best_threshold'])[0]\n",
    "print('\\nSingle sample -> probability:', float(prob), 'prediction_at_best_threshold:', int(pred))\n",
    "\n",
    "# small batch\n",
    "batch_probs = inference_service.predict_proba(X_test.iloc[:10])\n",
    "print('\\nBatch probs (first 10):', batch_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
